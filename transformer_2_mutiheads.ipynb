{
 "cells": [
  {
   "cell_type": "code",
   "id": "b92a0554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:01:06.176237Z",
     "start_time": "2025-05-14T15:01:05.709838Z"
    }
   },
   "source": [
    "import torch\n",
    "from sympy.codegen.fnodes import dimension\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "197298cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:10:01.045965Z",
     "start_time": "2025-05-14T15:10:01.039915Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.concat = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        # encodings_for_q: (batch, seq_len, d_model)\n",
    "        # encodings_for_k: (batch, seq_len, d_model)\n",
    "        # encodings_for_v: (batch, seq_len, d_model)\n",
    "        batch, seq_len, d_model = encodings_for_q.size()\n",
    "        n_d = self.d_model // self.n_head\n",
    "        Q = self.W_q(encodings_for_q)\n",
    "        K = self.W_k(encodings_for_k)\n",
    "        V = self.W_v(encodings_for_v)\n",
    "        Q = Q.view(batch, seq_len, self.n_head, n_d).permute(0, 2, 1, 3) # (batch, n_head, seq_len, d_model)\n",
    "        K = K.view(batch, seq_len, self.n_head, n_d).permute(0, 2, 1, 3) # (batch, n_head, seq_len, d_model)\n",
    "        V = V.view(batch, seq_len, self.n_head, n_d).permute(0, 2, 1, 3) # (batch, n_head, seq_len, d_model)\n",
    "        scaled_sims = Q@K.transpose(2, 3) / torch.sqrt(torch.tensor(n_d))\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask == 0, -1e9)\n",
    "        attention_percent = self.softmax(scaled_sims)\n",
    "        attention_scores = attention_percent@V # (batch, n_head, seq_len, d_model)\n",
    "        attention_scores = attention_scores.permute(0, 2, 1, 3).contiguous().view(batch, seq_len, d_model) # (batch, seq_len, d_model)\n",
    "\n",
    "        return self.concat(attention_scores)\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "66d8d4cb6f7e829a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:18:02.585712Z",
     "start_time": "2025-05-14T15:18:02.551467Z"
    }
   },
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.rand(128, 32, 512) # (batch_size, seq_len, d_model）\n",
    "multiHeadAttention = MultiHeadAttention(d_model=512, n_head=8)\n",
    "print(\"多头注意力的输出大小：\", mutiHeadAttention(x, x, x).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多头注意力的输出大小： torch.Size([128, 32, 512])\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
