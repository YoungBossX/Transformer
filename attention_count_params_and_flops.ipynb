{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f6722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "from thop import profile\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdc0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, bias, device):\n",
    "        \"\"\"\n",
    "        多头注意力机制的实现。 \n",
    "        Args:\n",
    "        hidden_size (int): 输入特征的维度，也即 hidden_state 的最后一维。\n",
    "        num_heads (int): 注意力头的数量。\n",
    "        dropout (float): dropout 的概率，默认为 0.0。 \n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size 必须能被 num_heads 整除\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads # 每个头的维度\n",
    "        \n",
    "        # 定义线性变换层，用于生成Q、K、V\n",
    "        self.query = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        self.out_projection = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        \n",
    "    def forward(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): 输入的 hidden_state，形状为 [batch_size, seq_len, hidden_size]。\n",
    "            attention_mask (torch.Tensor, optional): 注意力掩码，用于屏蔽某些位置，形状为 [batch_size, seq_len]。默认为 None。\n",
    "        Returns:\n",
    "             torch.Tensor: 注意力输出，形状为 [batch_size, seq_len, hidden_size]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "        \n",
    "        # 1. 通过线性层得到 Q, K, V\n",
    "        query = self.query(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        key = self.key(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        value = self.value(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # 2. 将 Q, K, V 拆分成多头\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 3. 计算注意力权重\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5) # [batch_size, num_heads， seq_len, seq_len]\n",
    "        \n",
    "        # 应用 attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf')) # attention_mask[:, None, None, :] 将掩码从 [batch_size, seq_len] 扩展为 [batch_size, 1, 1, seq_len]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 4. 计算上下文向量\n",
    "        context = torch.matmul(attention_weights, value)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 5. 将多头合并\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)  # [batch_size, seq_len, hidden_size]，contiguous()确保内存布局是连续的，为后续的view操作做准备\n",
    "        \n",
    "        # 6. 通过输出线性层\n",
    "        output = self.out_projection(context)  # [batch_size, seq_len, hidden_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f82339a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, group_size, dropout, bias, device):\n",
    "        \"\"\"\n",
    "        Grouped Query Attention 实现。\n",
    "        Args:\n",
    "            hidden_size (int): 输入特征的维度\n",
    "            num_heads (int): 查询头的数量。\n",
    "            group_size (int): 每个组中包含的查询头数量。\n",
    "            dropout (float): dropout 的概率。\n",
    "        \"\"\"\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        assert hidden_size % num_heads == 0, \"hidden_size 必须能被 num_heads 整除\"\n",
    "        assert num_heads % group_size == 0, \"num_heads 必须能被 group_size 整除\"\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.group_size = group_size\n",
    "        self.group_num = num_heads // group_size\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # 查询头\n",
    "        self.query = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        \n",
    "        # 键和值头（分组共享）\n",
    "        self.key = nn.Linear(hidden_size, self.group_num * self.head_dim, bias, device)\n",
    "        self.value = nn.Linear(hidden_size, self.group_num * self.head_dim, bias, device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_projection = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        \n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): 输入张量，形状为 [batch_size, seq_len, hidden_size]。\n",
    "            attention_mask (torch.Tensor, optional): 注意力掩码，形状为 [batch_size, seq_len]。\n",
    "        Returns:\n",
    "            torch.Tensor: 注意力输出，形状为 [batch_size, seq_len, hidden_size]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "        \n",
    "        # 1. 通过线性层得到 Q, K, V\n",
    "        query = self.query(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        key = self.key(hidden_state) # [batch_size, seq_len, group_num * head_dim]\n",
    "        value = self.value(hidden_state) # [batch_size, seq_len, group_num * head_dim]\n",
    "        \n",
    "        # 2. 将 Q, K, V 拆分成多头\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 3. K 和 V 扩展到 num_heads 个头\n",
    "        key = key.view(batch_size, seq_len, self.group_num, self.head_dim).transpose(1, 2) # [batch_size, group_num, seq_len, head_dim]\n",
    "        key = key.unsqueeze(2).expand(-1, -1, self.group_size, -1, -1).contiguous().view(batch_size, -1, seq_len, self.head_dim) # [batch_size, num_heads, seq_len, head_dim]\n",
    "        value = value.view(batch_size, seq_len, self.group_num, self.head_dim).transpose(1, 2) # [batch_size, group_num, seq_len, head_dim]\n",
    "        value = value.unsqueeze(2).expand(-1, -1, self.group_size, -1, -1).contiguous().view(batch_size, -1, seq_len, self.head_dim) # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 4. 计算注意力权重\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # 5. attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf'))\n",
    "            \n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 6. 计算上下文向量\n",
    "        context = torch.matmul(attention_weights, value) # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 7. 将多头合并\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size) # [batch_size, seq_len, hidden_size]\n",
    "         \n",
    "        # 8. 通过输出线性层\n",
    "        output = self.out_projection(context) # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0f4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, base, max_len):\n",
    "        \"\"\"\n",
    "        RoPE位置编码模块。\n",
    "        Args:\n",
    "            hidden_size (int): 模型维度\n",
    "            num_heads (int): 注意力头数量\n",
    "            base (int): 频率基值\n",
    "            max_len (int): 最大序列长度\n",
    "        \"\"\"\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.base = base\n",
    "        self.max_len = max_len\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.cos_pos_cache, self.sin_pos_cache = self._compute_pos_emb()\n",
    "    \n",
    "    def _compute_pos_emb(self):\n",
    "        \"\"\"\n",
    "        计算位置编码的余弦和正弦值。\n",
    "        Returns:\n",
    "            cos_pos (Tensor): 余弦位置编码\n",
    "            sin_pos (Tensor): 正弦位置编码\n",
    "        \"\"\"\n",
    "        theta_i = 1. / (self.base ** (torch.arange(0, self.head_dim, 2).float() / self.head_dim))\n",
    "        positions = torch.arange(self.max_len)\n",
    "        pos_emb = positions.unsqueeze(1) * theta_i.unsqueeze(0)\n",
    "        \n",
    "        cos_pos = pos_emb.sin().repeat_interleave(2, dim=-1)\n",
    "        sin_pos = pos_emb.cos().repeat_interleave(2, dim=-1)\n",
    "        \n",
    "        return cos_pos, sin_pos\n",
    "     \n",
    "    def forward(self, q):\n",
    "        \"\"\"\n",
    "        RoPE位置编码应用。\n",
    "        Args:\n",
    "            q (torch.Tensor): 输入张量 [bs, num_heads, seq_len, head_dim]\n",
    "        Returns:\n",
    "            torch.Tensor: 应用位置编码后的张量\n",
    "        \"\"\"\n",
    "        bs, seq_len = q.shape[0], q.shape[2]\n",
    "        cos_pos = self.cos_pos_cache[:seq_len].to(q.device) # [seq_len, head_dim]\n",
    "        sin_pos = self.sin_pos_cache[:seq_len].to(q.device) # [seq_len, head_dim]\n",
    "        \n",
    "        # 扩展维度以匹配batch和head维度\n",
    "        cos_pos = cos_pos.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, head_dim]\n",
    "        sin_pos = sin_pos.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, head_dim]\n",
    "        \n",
    "        # RoPE变换\n",
    "        q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1) # 奇偶交替\n",
    "        q2 = q2.reshape(q.shape).contiguous()\n",
    "        \n",
    "        return q * cos_pos + q2 * sin_pos\n",
    "    \n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, down_dim, up_dim, num_heads, rope_head_dim, base, max_len, dropout, bias, device):      \n",
    "        \"\"\"\n",
    "        Multi-Head Latent Attention 实现。\n",
    "        Args:\n",
    "            hidden_size (int): 输入特征维度。\n",
    "            down_dim (int): 降维后的维度。\n",
    "            up_dim (int): 升维后的维度。\n",
    "            num_heads (int): 注意力头数量。\n",
    "            rope_head_dim (int): RoPE编码的头维度。\n",
    "            dropout (float): ddropout概率。\n",
    "            bias (bool): 是否使用偏置。\n",
    "            device (str): 设备类型（'cpu'或'cuda'）。\n",
    "        \"\"\"\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.down_dim = down_dim\n",
    "        self.up_dim = up_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.rope_head_dim = rope_head_dim\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.v_head_dim = up_dim // num_heads\n",
    "        \n",
    "        # 降维投影\n",
    "        self.down_proj_kv = nn.Linear(hidden_size, down_dim, bias, device)\n",
    "        self.down_proj_q = nn.Linear(hidden_size, down_dim, bias, device)\n",
    "        \n",
    "        # 升维投影\n",
    "        self.up_proj_k = nn.Linear(down_dim, up_dim, bias, device)\n",
    "        self.up_proj_v = nn.Linear(down_dim, up_dim, bias, device)\n",
    "        self.up_proj_q = nn.Linear(down_dim, up_dim, bias, device)\n",
    "        \n",
    "        # 解耦Q/K投影\n",
    "        self.proj_qr = nn.Linear(down_dim, rope_head_dim * num_heads, bias, device)\n",
    "        self.proj_kr = nn.Linear(hidden_size, rope_head_dim, bias, device)\n",
    "        \n",
    "        # RoPE位置编码\n",
    "        self.rope_q = RotaryEmbedding(rope_head_dim * num_heads, num_heads, base, max_len)\n",
    "        self.rope_k = RotaryEmbedding(rope_head_dim, 1, base, max_len)\n",
    "        \n",
    "        # 输出层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_heads * self.v_head_dim, hidden_size, bias, device)\n",
    "        self.res_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, hidden_state, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        Args:\n",
    "            h (torch.Tensor): 输入张量 [batch_size, seq_len, hidden_size]\n",
    "            mask (torch.Tensor): 注意力掩码 [batch_size, seq_len]\n",
    "        Returns:\n",
    "            torch.Tensor: 输出张量 [bs, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "        \n",
    "        # 1. 低秩转换\n",
    "        c_t_kv = self.down_proj_kv(hidden_state) # [batch_size, seq_len, down_dim]\n",
    "        k_t_c = self.up_proj_k(c_t_kv) # [batch_size, seq_len, up_dim]\n",
    "        v_t_c = self.up_proj_v(c_t_kv) # [batch_size, seq_len, up_dim]\n",
    "        c_t_q = self.down_proj_q(hidden_state) # [batch_size, seq_len, down_dim]\n",
    "        q_t_c = self.up_proj_q(c_t_q) # [batch_size, seq_len, up_dim]\n",
    "        \n",
    "        # 2. 解耦Q/K处理\n",
    "        # RoPE投影处理\n",
    "        q_t_r = self.proj_qr(c_t_q) # [batch_size, seq_len, rope_head_dim * num_heads]\n",
    "        q_t_r = q_t_r.view(batch_size, seq_len, self.num_heads, self.rope_head_dim).transpose(1, 2) # [batch_size, num_heads, seq_len, rope_head_dim]\n",
    "        q_t_r = self.rope_q(q_t_r) # 应用RoPE编码\n",
    "        \n",
    "        k_t_r = self.proj_kr(hidden_state) # [batch_size, seq_len, rope_head_dim]\n",
    "        k_t_r = k_t_r.unsqueeze(1) # [batch_size, 1, seq_len, rope_head_dim]\n",
    "        k_t_r = self.rope_k(k_t_r) # 应用RoPE编码\n",
    "        \n",
    "        # 3. 注意力计算\n",
    "        # Q/K/V维度调整\n",
    "        q_t_c = q_t_c.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        q = torch.cat([q_t_c, q_t_r], dim=-1) # [batch_size, num_heads, seq_len, (up_dim+rope_head_dim)/num_heads]\n",
    "        \n",
    "        k_t_c = k_t_c.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        k_t_r = k_t_r.expand(batch_size, self.num_heads, seq_len, -1) # [batch_size, num_heads, seq_len, rope_head_dim]\n",
    "        k = torch.cat([k_t_c, k_t_r], dim=-1) # [batch_size, num_heads, seq_len, (up_dim+rope_head_dim)/num_heads]\n",
    "        \n",
    "        # 4. 计算注意力权重\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        scores = scores / (math.sqrt(self.head_dim) + math.sqrt(self.rope_head_dim))\n",
    "        \n",
    "        # 5. attention mask\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(mask[:, None, None, :] == 0, float('-inf')) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim=-1) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 6. V维度调整\n",
    "        v_t_c = v_t_c.view(batch_size, seq_len, self.num_heads, self.v_head_dim).transpose(1, 2) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        \n",
    "        # 7. 计算上下文向量\n",
    "        context = torch.matmul(attention_weights, v_t_c) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        \n",
    "        # 8. 合并多头\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1) # [batch_size, seq_len, num_heads * v_head_dim]\n",
    "        \n",
    "        # 9. 输出投影\n",
    "        output = self.fc(context) # [batch_size, seq_len, hidden_size]\n",
    "        output = self.res_dropout(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d6fe559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params_and_flops(module: nn.Module, input_shape: tuple, attention_mask: bool = False, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    统计指定模型模块的参数量和计算量(FLOPs)。\n",
    "    Args:\n",
    "        module: PyTorch 模块对象。\n",
    "        input_shape: 输入张量的形状 (元组形式, 不包含 batch 维度)。\n",
    "    Returns:\n",
    "        params_total: 总参数量。\n",
    "        flops_total: 总计算量。\n",
    "    \"\"\"\n",
    "    # 构造示例输入\n",
    "    dummy_input = torch.randn(2, *input_shape, device=device) # 添加 batch 维度\n",
    "    \n",
    "    # 计算参数量（单位：k）\n",
    "    params_total = sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "    # 计算计算量（单位：GFLOPs）\n",
    "    with redirect_stdout(open(os.devnull, \"w\")):\n",
    "        flops_total, _ = profile(module, inputs=(dummy_input, attention_mask))\n",
    "        \n",
    "    return params_total, flops_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========  Attention  Test  ==========\n",
      "MHA Output Shape: torch.Size([2, 10, 256])\n",
      "MHA Params: 327680, FLOPs: 5242880.0\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 示例\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    hidden_size = 256\n",
    "    num_heads = 8\n",
    "    group_size = 2\n",
    "    base=10000\n",
    "    max_len=512\n",
    "    down_dim=64\n",
    "    up_dim=128\n",
    "    rope_head_dim=26\n",
    "    dropout = 0.1\n",
    "    bias = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建一个随机的 hidden_state\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "    \n",
    "    # 创建一个 attention mask (可选)\n",
    "    attention_mask = torch.ones(batch_size, seq_len, device=device)\n",
    "    attention_mask[:, 5:] = 0\n",
    "    \n",
    "    print(\"==\" * 5, \" Attention  Test \", \"==\" * 5)\n",
    "    \n",
    "    # 创建一个 MHA 实例\n",
    "    mha = MultiHeadAttention(hidden_size, num_heads, dropout, bias, device)\n",
    "    \n",
    "    # 通过 MHA 层\n",
    "    output = mha(hidden_state, attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"MHA Output Shape:\", output.shape)\n",
    "    \n",
    "    # 统计参数量和计算量\n",
    "    mha_params, mha_flops = count_params_and_flops(mha, (seq_len, hidden_size), attention_mask, device)\n",
    "    print(f\"MHA Params: {mha_params}, FLOPs: {mha_flops}\")\n",
    "    \n",
    "    print(\"===\" * 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8c1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========  Attention  Test  ==========\n",
      "GQA Output Shape: torch.Size([2, 10, 256])\n",
      "GQA Params: 196608, FLOPs: 3932160.0\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 示例\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    hidden_size = 256\n",
    "    num_heads = 8\n",
    "    group_size = 2\n",
    "    dropout = 0.1\n",
    "    bias = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建一个随机的 hidden_state\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "    \n",
    "    # 创建一个 attention mask (可选)\n",
    "    attention_mask = torch.ones(batch_size, seq_len, device=device)\n",
    "    attention_mask[:, 5:] = 0\n",
    "    \n",
    "    print(\"==\" * 5, \" Attention  Test \", \"==\" * 5)\n",
    "    \n",
    "    # 创建一个 GQA 实例\n",
    "    gqa = GroupedQueryAttention(hidden_size, num_heads, group_size, dropout, bias, device)\n",
    "    \n",
    "    # 通过 GQA 层\n",
    "    output = gqa(hidden_state, attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"GQA Output Shape:\", output.shape)\n",
    "    \n",
    "    # 统计参数量和计算量\n",
    "    gqa_params, gqa_flops = count_params_and_flops(gqa, (seq_len, hidden_size), attention_mask, device)\n",
    "    print(f\"GQA Params: {gqa_params}, FLOPs: {gqa_flops}\")\n",
    "    \n",
    "    print(\"===\" * 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aaa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========  Attention  Test  ==========\n",
      "MLA Output Shape: torch.Size([2, 10, 256])\n",
      "MlA Params: 110080, FLOPs: 2201600.0\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 示例\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    hidden_size = 256\n",
    "    num_heads = 8\n",
    "    base=10000\n",
    "    max_len=512\n",
    "    down_dim=64\n",
    "    up_dim=128\n",
    "    rope_head_dim=26\n",
    "    dropout = 0.1\n",
    "    bias = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建一个随机的 hidden_state\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "    \n",
    "    # 创建一个 attention mask (可选)\n",
    "    attention_mask = torch.ones(batch_size, seq_len, device=device)\n",
    "    attention_mask[:, 5:] = 0\n",
    "    \n",
    "    print(\"==\" * 5, \" Attention  Test \", \"==\" * 5)\n",
    "    \n",
    "    # 创建一个 MLA 实例\n",
    "    mla = MultiHeadLatentAttention(hidden_size, down_dim, up_dim, num_heads, rope_head_dim, base, max_len, dropout, bias, device)\n",
    "    \n",
    "    # 通过 MLA 层\n",
    "    output = mla(hidden_state, attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"MLA Output Shape:\", output.shape)\n",
    "    \n",
    "    # 统计参数量和计算量\n",
    "    mla_params, mla_flops = count_params_and_flops(mla, (seq_len, hidden_size), attention_mask, device)\n",
    "    print(f\"MlA Params: {mla_params}, FLOPs: {mla_flops}\")\n",
    "    \n",
    "    print(\"===\" * 13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
