{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce27373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b967d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, bias, device):\n",
    "        \"\"\"\n",
    "        多头注意力机制的实现。 \n",
    "        Args:\n",
    "        hidden_size (int): 输入特征的维度，也即 hidden_state 的最后一维。\n",
    "        num_heads (int): 注意力头的数量。\n",
    "        dropout (float): dropout 的概率，默认为 0.0。 \n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size 必须能被 num_heads 整除\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads # 每个头的维度\n",
    "        \n",
    "        # 定义线性变换层，用于生成Q、K、V\n",
    "        self.query = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        self.out_projection = nn.Linear(hidden_size, hidden_size, bias, device)\n",
    "        \n",
    "    def forward(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        Args:\n",
    "            hidden_state (torch.Tensor): 输入的 hidden_state，形状为 [batch_size, seq_len, hidden_size]。\n",
    "            attention_mask (torch.Tensor, optional): 注意力掩码，用于屏蔽某些位置，形状为 [batch_size, seq_len]。默认为 None。\n",
    "        Returns:\n",
    "             torch.Tensor: 注意力输出，形状为 [batch_size, seq_len, hidden_size]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "        \n",
    "        # 1. 通过线性层得到 Q, K, V\n",
    "        query = self.query(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        key = self.key(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        value = self.value(hidden_state) # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # 2. 将 Q, K, V 拆分成多头\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 3. 计算注意力权重\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5) # [batch_size, num_heads， seq_len, seq_len]\n",
    "        \n",
    "        # 应用 attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf')) # attention_mask[:, None, None, :] 将掩码从 [batch_size, seq_len] 扩展为 [batch_size, 1, 1, seq_len]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 4. 计算上下文向量\n",
    "        context = torch.matmul(attention_weights, value)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 5. 将多头合并\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)  # [batch_size, seq_len, hidden_size]，contiguous()确保内存布局是连续的，为后续的view操作做准备\n",
    "        \n",
    "        # 6. 通过输出线性层\n",
    "        output = self.out_projection(context)  # [batch_size, seq_len, hidden_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34d548b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__ (self, d, base):\n",
    "        super(RotaryPositionalEmbeddings, self).__init__()\n",
    "        self.d = d\n",
    "        self.base = base\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "    \n",
    "    def _build_cache(self, x):\n",
    "        if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "            return \n",
    "        \n",
    "        seq_len = x.shape[0]\n",
    "        \n",
    "        theta = 1. / (self.base ** (torch.arange(0, self.d, 2, device=x.device).float() / self.d))\n",
    "        \n",
    "        seq_idx = torch.arange(seq_len, device=x.device).float()\n",
    "\n",
    "        idx_theta = torch.einsum('n, d->nd', seq_idx, theta)\n",
    "\n",
    "        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "\n",
    "        self.cos_cached = torch.cos(idx_theta2)[: , None, None, :]\n",
    "        self.sin_cached = torch.sin(idx_theta2)[: , None, None, :]\n",
    "\n",
    "    def _neg_half(self, x):\n",
    "        d_2 = self.d // 2\n",
    "        return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self._build_cache(x)\n",
    "        x_rope, x_pass = x[:, :, :, :self.d], x[:, :, :, self.d:]\n",
    "\n",
    "        neg_half = self._neg_half(x_rope)\n",
    "\n",
    "        x_rope = (x_rope * self.cos_cached[:x.shape[0]]) + (neg_half * self.sin_cached[:x.shape[0]])\n",
    "\n",
    "        return torch.cat([x_rope, x_pass], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f9878a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPEMultiHeadAttention(MultiHeadAttention):\n",
    "    def __init__(self, heads, d_model, rope_percentage, base, dropout_prob, bias, device):\n",
    "        super(RotaryPEMultiHeadAttention, self).__init__(d_model, heads, dropout_prob, bias, device)\n",
    "        self.head_dim = d_model // heads\n",
    "        d_rope = int(self.head_dim * rope_percentage)\n",
    "        self.query_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n",
    "        self.key_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n",
    "    \n",
    "    def get_scores(self, query, key):\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', self.query_rotary_pe(query), self.key_rotary_pe(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45918a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 1, 4])\n",
      "tensor([[[[  1.0000,   2.0000,   3.0000,   4.0000]]],\n",
      "\n",
      "\n",
      "        [[[ -2.8876,   4.9298,   6.6077,   7.0496]]],\n",
      "\n",
      "\n",
      "        [[[-11.0967,   7.7984,   2.6198,  10.1580]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    d_model = 4\n",
    "    base = 10000\n",
    "    rope_percentage = 0.5\n",
    "    dropout_prob = 0\n",
    "    bias = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]], dtype=torch.float, device='cuda')\n",
    "    x = x[:, None, None, :]\n",
    "    print(x.shape)\n",
    "    rotary_pe = RotaryPositionalEmbeddings(d_model, base)\n",
    "    print(rotary_pe(x))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
