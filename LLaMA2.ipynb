{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61018620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class ModelConfig(PretrainedConfig):\n",
    "    model_type = \"Tiny-K\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int =768,\n",
    "        n_layers: int =12,\n",
    "        n_heads: int =16,\n",
    "        n_kv_heads: int = 8,\n",
    "        vocab_size: int = 6144,\n",
    "        hidden_dim: int = None,\n",
    "        multiple_of: int = 64, \n",
    "        norm_eps: float = 1e-5,\n",
    "        max_seq_len: int = 512,\n",
    "        dropout: float = 0.0,\n",
    "        flash_attn: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norm_eps = norm_eps\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "        self.flash_attn = flash_attn\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6e25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # weight是一个可学习的参数，全部初始化为1\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        # 计算RMSNorm的核心部分\n",
    "        # x.pow(2).mean(-1, keepdim=True)计算了输入x的平方的均值\n",
    "        # torch.rsqrt是平方根的倒数，这样就得到了RMSNorm的分母部分，再加上eps防止分母为0\n",
    "        # 最后乘以x，得到RMSNorm的结果\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward函数是模型的前向传播\n",
    "        # 首先将输入x转为float类型，然后进行RMSNorm，最后再转回原来的数据类型\n",
    "        # 最后乘以weight，这是RMSNorm的一个可学习的缩放因子\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "args = ModelConfig()\n",
    "\n",
    "norm = RMSNorm(args.dim, args.norm_eps)\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "output = norm(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9631a9",
   "metadata": {},
   "source": [
    "下面代码中：\n",
    "\n",
    "获取输入张量的形状：首先，代码通过 x.shape 获取输入张量的形状，包括批量大小（bs）、序列长度（slen）、键/值对头的数量（n_kv_heads）以及每个头的维度大小（head_dim）。\n",
    "\n",
    "然后，检查重复次数：接着，代码检查重复次数 n_rep 是否为1。如果是1，则说明不需要对键和值进行重复，直接返回原始张量 x。\n",
    "\n",
    "最后，扩展和重塑张量：\n",
    "\n",
    "在第三个维度（即键/值对头的维度）之后添加一个新的维度，形成 x[:, :, :, None, :]。\n",
    "使用 expand 方法将新添加的维度扩展到 n_rep 大小，实现键/值对的重复效果。\n",
    "最后，通过 reshape 方法重新塑形，将扩展后的维度合并回键/值对头的数量中，即 x.reshape(bs, slen, n_kv_heads * n_rep, head_dim)，这样最终的张量形状就达到了与查询维度一致的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    # 获取输入张量的形状：批量大小、序列长度、键/值对头的数量、每个头的维度大小\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "\n",
    "    # 如果重复次数为1，则不需要重复，直接返回原始张量\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    # 对张量进行扩展和重塑操作以重复键值对\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # 在第四个维度（头的维度前）添加一个新的维度\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # 将新添加的维度扩展到n_rep大小，实现重复的效果\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)  # 重新塑形，合并键/值对头的数量和重复次数的维度\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff13dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 24]) torch.Size([50, 24])\n",
      "torch.Size([1, 50, 6, 48]) torch.Size([1, 50, 6, 48])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "# 注意：此处的dim应为 dim//n_head，因为是对每个head进行旋转嵌入\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    # torch.arange(0, dim, 2)[: (dim // 2)].float()生成了一个从0开始，步长为2的序列，长度为dim的一半\n",
    "    # 然后每个元素除以dim，再取theta的倒数，得到频率\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 生成一个从0到end的序列，长度为end\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    # 计算外积，得到一个二维矩阵，每一行是t的元素乘以freqs的元素\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # 计算频率的余弦值，得到实部\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    # 计算频率的正弦值，得到虚部\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    # 获取x的维度数\n",
    "    ndim = x.ndim\n",
    "    # 断言，确保1在x的维度范围内\n",
    "    assert 0 <= 1 < ndim\n",
    "    # 断言，确保freqs_cis的形状与x的第二维和最后一维相同\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    # 构造一个新的形状，除了第二维和最后一维，其他维度都为1，这样做是为了能够将freqs_cis与x进行广播操作\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    # 将freqs_cis调整为新的形状，并返回\n",
    "    return freqs_cis.view(shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]: \n",
    "    # 将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    # 重新塑形频率张量以进行广播\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "    # 应用旋转，分别计算旋转后的实部和虚部\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    # 将最后两个维度合并，并还原为原始张量的形状\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "xq = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "xk = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "# 使用 precompute_freqs_cis 函数获取 sin和cos\n",
    "cos, sin = precompute_freqs_cis(288//6, 50)\n",
    "print(cos.shape, sin.shape)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, cos, sin)\n",
    "print(xq_out.shape, xk_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
