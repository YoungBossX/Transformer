{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44e3844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, base, max_len):\n",
    "        \"\"\"\n",
    "        RoPE位置编码模块。\n",
    "        Args:\n",
    "            hidden_size (int): 模型维度\n",
    "            num_heads (int): 注意力头数量\n",
    "            base (int): 频率基值\n",
    "            max_len (int): 最大序列长度\n",
    "        \"\"\"\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.base = base\n",
    "        self.max_len = max_len\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.cos_pos_cache, self.sin_pos_cache = self._compute_pos_emb()\n",
    "    \n",
    "    def _compute_pos_emb(self):\n",
    "        \"\"\"\n",
    "        计算位置编码的余弦和正弦值。\n",
    "        Returns:\n",
    "            cos_pos (Tensor): 余弦位置编码\n",
    "            sin_pos (Tensor): 正弦位置编码\n",
    "        \"\"\"\n",
    "        theta_i = 1. / (self.base ** (torch.arange(0, self.head_dim, 2).float() / self.head_dim))\n",
    "        positions = torch.arange(self.max_len)\n",
    "        pos_emb = positions.unsqueeze(1) * theta_i.unsqueeze(0)\n",
    "        \n",
    "        cos_pos = pos_emb.sin().repeat_interleave(2, dim=-1)\n",
    "        sin_pos = pos_emb.cos().repeat_interleave(2, dim=-1)\n",
    "        \n",
    "        return cos_pos, sin_pos\n",
    "     \n",
    "    def forward(self, q):\n",
    "        \"\"\"\n",
    "        RoPE位置编码应用。\n",
    "        Args:\n",
    "            q (torch.Tensor): 输入张量 [bs, num_heads, seq_len, head_dim]\n",
    "        Returns:\n",
    "            torch.Tensor: 应用位置编码后的张量\n",
    "        \"\"\"\n",
    "        bs, seq_len = q.shape[0], q.shape[2]\n",
    "        cos_pos = self.cos_pos_cache[:seq_len].to(q.device) # [seq_len, head_dim]\n",
    "        sin_pos = self.sin_pos_cache[:seq_len].to(q.device) # [seq_len, head_dim]\n",
    "        \n",
    "        # 扩展维度以匹配batch和head维度\n",
    "        cos_pos = cos_pos.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, head_dim]\n",
    "        sin_pos = sin_pos.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, head_dim]\n",
    "        \n",
    "        # RoPE变换\n",
    "        q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1) # 奇偶交替\n",
    "        q2 = q2.reshape(q.shape).contiguous()\n",
    "        \n",
    "        return q * cos_pos + q2 * sin_pos\n",
    "    \n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, down_dim, up_dim, num_heads, rope_head_dim, base, max_len, dropout, bias, device):      \n",
    "        \"\"\"\n",
    "        Multi-Head Latent Attention 实现。\n",
    "        Args:\n",
    "            hidden_size (int): 输入特征维度。\n",
    "            down_dim (int): 降维后的维度。\n",
    "            up_dim (int): 升维后的维度。\n",
    "            num_heads (int): 注意力头数量。\n",
    "            rope_head_dim (int): RoPE编码的头维度。\n",
    "            dropout (float): ddropout概率。\n",
    "            bias (bool): 是否使用偏置。\n",
    "            device (str): 设备类型（'cpu'或'cuda'）。\n",
    "        \"\"\"\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.down_dim = down_dim\n",
    "        self.up_dim = up_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.rope_head_dim = rope_head_dim\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.v_head_dim = up_dim // num_heads\n",
    "        \n",
    "        # 降维投影\n",
    "        self.down_proj_kv = nn.Linear(hidden_size, down_dim, bias, device)\n",
    "        self.down_proj_q = nn.Linear(hidden_size, down_dim, bias, device)\n",
    "        \n",
    "        # 升维投影\n",
    "        self.up_proj_k = nn.Linear(down_dim, up_dim, bias, device)\n",
    "        self.up_proj_v = nn.Linear(down_dim, up_dim, bias, device)\n",
    "        self.up_proj_q = nn.Linear(down_dim, up_dim, bias, device)\n",
    "        \n",
    "        # 解耦Q/K投影\n",
    "        self.proj_qr = nn.Linear(down_dim, rope_head_dim * num_heads, bias, device)\n",
    "        self.proj_kr = nn.Linear(hidden_size, rope_head_dim, bias, device)\n",
    "        \n",
    "        # RoPE位置编码\n",
    "        self.rope_q = RotaryEmbedding(rope_head_dim * num_heads, num_heads, base, max_len)\n",
    "        self.rope_k = RotaryEmbedding(rope_head_dim, 1, base, max_len)\n",
    "        \n",
    "        # 输出层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_heads * self.v_head_dim, hidden_size, bias, device)\n",
    "        self.res_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, hidden_state, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        Args:\n",
    "            h (torch.Tensor): 输入张量 [batch_size, seq_len, hidden_size]\n",
    "            mask (torch.Tensor): 注意力掩码 [batch_size, seq_len]\n",
    "        Returns:\n",
    "            torch.Tensor: 输出张量 [bs, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "        \n",
    "        # 1. 低秩转换\n",
    "        c_t_kv = self.down_proj_kv(hidden_state) # [batch_size, seq_len, down_dim]\n",
    "        k_t_c = self.up_proj_k(c_t_kv) # [batch_size, seq_len, up_dim]\n",
    "        v_t_c = self.up_proj_v(c_t_kv) # [batch_size, seq_len, up_dim]\n",
    "        c_t_q = self.down_proj_q(hidden_state) # [batch_size, seq_len, down_dim]\n",
    "        q_t_c = self.up_proj_q(c_t_q) # [batch_size, seq_len, up_dim]\n",
    "        \n",
    "        # 2. 解耦Q/K处理\n",
    "        # RoPE投影处理\n",
    "        q_t_r = self.proj_qr(c_t_q) # [batch_size, seq_len, rope_head_dim * num_heads]\n",
    "        q_t_r = q_t_r.view(batch_size, seq_len, self.num_heads, self.rope_head_dim).transpose(1, 2) # [batch_size, num_heads, seq_len, rope_head_dim]\n",
    "        q_t_r = self.rope_q(q_t_r) # 应用RoPE编码\n",
    "        \n",
    "        k_t_r = self.proj_kr(hidden_state) # [batch_size, seq_len, rope_head_dim]\n",
    "        k_t_r = k_t_r.unsqueeze(1) # [batch_size, 1, seq_len, rope_head_dim]\n",
    "        k_t_r = self.rope_k(k_t_r) # 应用RoPE编码\n",
    "        \n",
    "        # 3. 注意力计算\n",
    "        # Q/K/V维度调整\n",
    "        q_t_c = q_t_c.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        q = torch.cat([q_t_c, q_t_r], dim=-1) # [batch_size, num_heads, seq_len, (up_dim+rope_head_dim)/num_heads]\n",
    "        \n",
    "        k_t_c = k_t_c.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        k_t_r = k_t_r.expand(batch_size, self.num_heads, seq_len, -1) # [batch_size, num_heads, seq_len, rope_head_dim]\n",
    "        k = torch.cat([k_t_c, k_t_r], dim=-1) # [batch_size, num_heads, seq_len, (up_dim+rope_head_dim)/num_heads]\n",
    "        \n",
    "        # 4. 计算注意力权重\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        scores = scores / (math.sqrt(self.head_dim) + math.sqrt(self.rope_head_dim))\n",
    "        \n",
    "        # 5. attention mask\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(mask[:, None, None, :] == 0, float('-inf')) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim=-1) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 6. V维度调整\n",
    "        v_t_c = v_t_c.view(batch_size, seq_len, self.num_heads, self.v_head_dim).transpose(1, 2) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        \n",
    "        # 7. 计算上下文向量\n",
    "        context = torch.matmul(attention_weights, v_t_c) # [batch_size, num_heads, seq_len, v_head_dim]\n",
    "        \n",
    "        # 8. 合并多头\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1) # [batch_size, seq_len, num_heads * v_head_dim]\n",
    "        \n",
    "        # 9. 输出投影\n",
    "        output = self.fc(context) # [batch_size, seq_len, hidden_size]\n",
    "        output = self.res_dropout(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "639accef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 示例\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    hidden_size = 256\n",
    "    num_heads = 8\n",
    "    base=10000\n",
    "    max_len=512\n",
    "    down_dim=64\n",
    "    up_dim=128\n",
    "    rope_head_dim=26\n",
    "    dropout = 0.1\n",
    "    bias = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建一个 MLA 实例\n",
    "    mla = MultiHeadLatentAttention(hidden_size, down_dim, up_dim, num_heads, rope_head_dim, base, max_len, dropout, bias, device)\n",
    "    \n",
    "    # 创建一个随机的 hidden_state\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "    \n",
    "    # 创建一个 attention mask (可选)\n",
    "    attention_mask = torch.ones(batch_size, seq_len, device=device)\n",
    "    attention_mask[:, 5:] = 0 # 屏蔽掉每个 batch 中 seq_len 的后 5 个位置\n",
    "    \n",
    "    # 通过 MLA 层\n",
    "    output = mla(hidden_state, attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"输出形状:\", output.shape) # 应输出 torch.Size([2, 10, 256])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
