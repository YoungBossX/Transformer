{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                src_pad_idx,\n",
    "                trg_pad_idx,\n",
    "                encoder_voc_size,\n",
    "                decoder_voc_size,\n",
    "                max_len,\n",
    "                d_model,\n",
    "                n_head,\n",
    "                ffn_hidden,\n",
    "                n_layer,\n",
    "                dropout,\n",
    "                device,\n",
    "                 ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            encoder_voc_size,\n",
    "            max_len,\n",
    "            d_model,\n",
    "            n_head,\n",
    "            ffn_hidden,\n",
    "            n_layer,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            decoder_voc_size,\n",
    "            max_len,\n",
    "            d_model,\n",
    "            n_head,\n",
    "            ffn_hidden,\n",
    "            n_layer,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_pad_mask(self, Q, K, pad_idx_q, pad_idx_k):\n",
    "        len_q = Q.size(1)\n",
    "        len_k = K.size(1)\n",
    "        Q = Q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3) # (B, 1, Lq, 1)\n",
    "        Q = Q.repeat(1, 1, 1, len_k) \n",
    "        K = K.ne(pad_idx_k).unsqueeze(1).unsqueeze(2) # (B, 1, 1, Lk)\n",
    "        K = K.repeat(1, 1, len_q, 1)\n",
    "        mask = Q & K\n",
    "        return mask\n",
    "    \n",
    "    def make_causal_mask(self, Q, K):\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * self.make_casual_mask(trg, trg)\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(trg, encoder_output, trg_mask, src_mask)\n",
    "        encoder = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, encoder, trg_mask, src_mask)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
